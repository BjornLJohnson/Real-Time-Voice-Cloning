{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjorn/dev/ECE_228/228/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/bjorn/dev/ECE_228/228/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from encoder import inference as styleEncoder\n",
    "from vocoder import inference as vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = None # type: VoiceImpersonator\n",
    "_hparams = None\n",
    "_device = None\n",
    "_use_cuda = None\n",
    "_train_audio_transforms = None\n",
    "_valid_audio_transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_path=None, save_model_path=None, batch_size=3, epochs=1):\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "    _use_cuda = torch.cuda.is_available()\n",
    "    _device = torch.device(\"cuda\" if _use_cuda else \"cpu\")\n",
    "    warm_start=True\n",
    "\n",
    "    _hparams = {\n",
    "        \"n_cnn_layers\": 2,\n",
    "        \"n_rnn_layers\": 4,\n",
    "        \"rnn_dim\": 256,\n",
    "#         \"n_class\": 29,\n",
    "#         \"n_feats\": 64,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"save_model_path\": save_model_path\n",
    "    }\n",
    "\n",
    "    _model = VoiceImpersonator(\n",
    "    _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "    _hparams['stride'], _hparams['dropout']\n",
    "    ).to(_device)\n",
    "\n",
    "    if model_path is not None:\n",
    "        _model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "    print(_model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in _model.parameters()]))\n",
    "    load_transforms()\n",
    "    \n",
    "#     vocoder.load_model(Path(\"vocoder/saved_models/pretrained/pretrained.pt\"))\n",
    "    styleEncoder.load_model(Path(\"encoder/saved_models/pretrained.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all components of encoder network\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, stride=2, dropout=0.1):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        \n",
    "        # Single convolutional layer for basic heirarchal feature extraction\n",
    "        # Conv2D(in_channels, out_channels, kernel_size, stride, padding, dilation....)\n",
    "        self.Convolutional_Feature_Extraction = nn.Conv2d(1, 32, 3, stride=stride, padding=1)\n",
    "\n",
    "        # n_cnn Residual Convolutional Layers for deeper feature extraction\n",
    "        self.Residual_CNN_Blocks = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=64)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "                             \n",
    "        # Single fully connected layer, 2048 inputs (64 features * 32 filters), rnn_dim outputs\n",
    "        # Somewhat misleading, I believe this outputs a batch of matrices\n",
    "        self.Feature_Downsampling = nn.Linear(2048, rnn_dim)\n",
    "        \n",
    "        self.Recurrent_Block = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input is a mel spectrogram\n",
    "        # Input is of shape (batch, channels=1, mel_features=128, timesteps)\n",
    "        x = self.Convolutional_Feature_Extraction(x)\n",
    "        x = self.Residual_CNN_Blocks(x)\n",
    "        \n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        \n",
    "        x = self.Feature_Downsampling(x)\n",
    "        x = self.Recurrent_Block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)   \n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define componenets of Synthesizer network\n",
    "class Synthesizer(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, stride=2, dropout=0.1):\n",
    "        super(Synthesizer, self).__init__()\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size=768, hidden_size=1024, num_layers=1)\n",
    "        \n",
    "        self.Linear_Projection = nn.Linear(in_features=1024, out_features=80)\n",
    "        \n",
    "        self.PostNet = PostNet(in_channels=1, out_channels=256, kernel=(1,5),\n",
    "                               stride=1, padding=(0,2), dropout=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input is a batch of feature matrices from encoder network concatenated w/ style embeddings\n",
    "        # Input is of shape (batch, \"timesteps\", features=768)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x = self.Linear_Projection(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = self.PostNet(x)\n",
    "        print(\"After PostNet\")\n",
    "        print(x.size())\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout):\n",
    "        super(PreNet, self).__init__()\n",
    "\n",
    "        self.fully_connected1 = nn.Linear(input_dim, 256)\n",
    "        self.fully_connected2 = nn.Linear(256, 256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fully_connected1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fully_connected2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "    \n",
    "class PostNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, padding, dropout):\n",
    "        super(PostNet, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n",
    "        self.cnn3 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n",
    "        self.cnn4 = nn.Conv2d(out_channels, 1, kernel, stride, padding)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.cnn4(x)\n",
    "\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge encoder and synthesizer into full network\n",
    "class VoiceImpersonator(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, stride=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=ContentEncoder(\n",
    "            _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "            _hparams['stride'], _hparams['dropout']\n",
    "            )\n",
    "        \n",
    "        self.synthesizer=Synthesizer(\n",
    "            _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "            _hparams['stride'], _hparams['dropout']\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input is a 2 element list, containing a batch of spectrograms and a batch of style embeddings\n",
    "        # Extract style embeddings from input tensor, remove from list\n",
    "        embeds = x[1]\n",
    "        x = x[0]\n",
    "        \n",
    "        print(\"Input Spectrograms Shape\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        print(\"Style Embeddings Shape\")\n",
    "        print(embeds.shape)\n",
    "        \n",
    "        # Compute content encoding\n",
    "        x = self.encoder(x)\n",
    "        print(\"Content Encodings Shape\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        styleEmbeds = torch.empty((_hparams['batch_size'], x.shape[1], 256)).to(_device)\n",
    "                                            \n",
    "        for i in range(0, _hparams['batch_size']-1):\n",
    "            styleEmbeds[i,:]=embeds[i,:].repeat(1,x.shape[1],1)\n",
    "                                            \n",
    "        print(styleEmbeds.shape)\n",
    "\n",
    "        # Concatenate content with style embedding, now size (batch, timesteps, 768)\n",
    "        x = torch.cat((x, styleEmbeds), 2)\n",
    "        print(\"Concatenated embedding shape\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Synthesize a spectrogram from the combined embeddings\n",
    "        x = self.synthesizer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage data preprocessing (creating spectrograms, transforms, etc)\n",
    "def load_transforms():\n",
    "    global _train_audio_transforms, _valid_audio_transforms\n",
    "    _train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "    )\n",
    "\n",
    "    _valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "    \n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    global _train_audio_transforms, _valid_audio_transforms\n",
    "    spectrograms = []\n",
    "#     styles = []\n",
    "\n",
    "    # waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id\n",
    "    for (waveform, sample_rate, _, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = _train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = _valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "#         preprocessed_wav = styleEncoder.preprocess_wav(waveform, sample_rate)\n",
    "#         styles.append(torch.from_numpy(styleEncoder.embed_utterance(preprocessed_wav)))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "\n",
    "    \n",
    "    return spectrograms#, styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Behavior       \n",
    "def train():\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded, call load_model() before training\")\n",
    "\n",
    "    dataset_dir = os.path.expanduser(\"~/dev/datasets/Libri\")\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        print(\"Running\")\n",
    "        os.makedirs(dataset_dir)\n",
    "\n",
    "    train_url=\"train-clean-100\"\n",
    "    test_url=\"test-clean\"\n",
    "\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(dataset_dir, url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(dataset_dir, url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if _use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=_hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=_hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "    optimizer = optim.AdamW(_model.parameters(), _hparams['learning_rate'])\n",
    "#     criterion = nn.CTCLoss(blank=28).to(_device)\n",
    "            \n",
    "                             \n",
    "                             \n",
    "    # NEED A NEW LOSS\n",
    "                             \n",
    "                             \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=_hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=_hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "          \n",
    "        \n",
    "        \n",
    "    # Currently only one style encoding, generating all speech in same voice\n",
    "    preprocessed_wav = styleEncoder.preprocess_wav(Path(\"data/styleAudio/rand1.flac\"))\n",
    "    styleEmbed = torch.from_numpy(styleEncoder.embed_utterance(preprocessed_wav))\n",
    "    styleEmbed = styleEmbed.repeat(_hparams['batch_size'],1).to(_device)\n",
    "    \n",
    "    _model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for epoch in range(1, _hparams['epochs'] + 1):\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms = _data\n",
    "            spectrograms = spectrograms.to(_device)\n",
    "            x = [spectrograms, styleEmbed]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = _model(x)  # (batch, timesteps, frequencies)\n",
    "#             print(output.shape)\n",
    "#             output = F.log_softmax(output, dim=2)\n",
    "#             output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                             \n",
    "#             # NEED TO IMPLEMENT NEW LOSS AND GRADIENT PROPOGATION\n",
    "                             \n",
    "# #             loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#             loss.backward()\n",
    "\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "#                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, batch_idx * len(spectrograms), data_len,\n",
    "#                     100. * batch_idx / len(train_loader), loss.item()))\n",
    "                             \n",
    "#         # Calculate validation statistics and save after each epoch                 \n",
    "#         test(_model, _device, test_loader, criterion, epoch)\n",
    "#         torch.save(_model.state_dict(), _hparams['save_model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Validation Metrics and Behavior\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms = _data \n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "#             output = F.log_softmax(output, dim=2)\n",
    "#             output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            # Print loss information\n",
    "            \n",
    "#             loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#             test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated, leaving in for possibility of code reuse in development             \n",
    "def encode(input_file_path):\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded, call load_model() and truncate_model() before encoding\")\n",
    "\n",
    "    # Currently only one style encoding, generating all speech in same voice\n",
    "    preprocessed_wav = styleEncoder.preprocess_wav(Path(\"data/styleAudio/rand1.flac\"))\n",
    "    styleEmbeds = torch.from_numpy(styleEncoder.embed_utterance(preprocessed_wav)).unsqueeze(0).to(_device)\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(input_file_path, normalization=True)\n",
    "    input_data = [[waveform, None, None, None, None, None]]\n",
    "    spectrograms = data_processing(input_data, 'valid').to(_device)\n",
    "    x = [spectrograms, styleEmbeds]\n",
    "    \n",
    "    _model.eval()\n",
    "    output=_model(x)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoiceImpersonator(\n",
      "  (encoder): ContentEncoder(\n",
      "    (Convolutional_Feature_Extraction): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (Residual_CNN_Blocks): Sequential(\n",
      "      (0): ResidualCNN(\n",
      "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (layer_norm2): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualCNN(\n",
      "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (layer_norm2): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Feature_Downsampling): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (Recurrent_Block): Sequential(\n",
      "      (0): BidirectionalGRU(\n",
      "        (BiGRU): GRU(256, 256, batch_first=True, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (synthesizer): Synthesizer(\n",
      "    (LSTM): LSTM(768, 1024)\n",
      "    (Linear_Projection): Linear(in_features=1024, out_features=80, bias=True)\n",
      "    (PostNet): PostNet(\n",
      "      (cnn1): Conv2d(1, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn2): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn3): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn4): Conv2d(256, 1, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 12992529\n",
      "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
      "Input Spectrograms Shape\n",
      "torch.Size([3, 1, 128, 1364])\n",
      "Style Embeddings Shape\n",
      "torch.Size([3, 256])\n",
      "Content Encodings Shape\n",
      "torch.Size([3, 682, 512])\n",
      "torch.Size([3, 682, 256])\n",
      "Concatenated embedding shape\n",
      "torch.Size([3, 682, 768])\n",
      "After PostNet\n",
      "torch.Size([3, 1, 682, 80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjorn/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Spectrograms Shape\n",
      "torch.Size([3, 1, 128, 1213])\n",
      "Style Embeddings Shape\n",
      "torch.Size([3, 256])\n",
      "Content Encodings Shape\n",
      "torch.Size([3, 607, 512])\n",
      "torch.Size([3, 607, 256])\n",
      "Concatenated embedding shape\n",
      "torch.Size([3, 607, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 3.95 GiB total capacity; 2.09 GiB already allocated; 89.06 MiB free; 2.14 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0bc36aeb56d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load a fresh model, untrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# output = encode(\"./data/contentAudio/40-222-0030.flac\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0227b8f90f16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, timesteps, frequencies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;31m#             print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#             output = F.log_softmax(output, dim=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b336a9151ad5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Synthesize a spectrogram from the combined embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-138cffe4c638>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPostNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After PostNet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-138cffe4c638>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# (batch, channel, feature, time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mtanh\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1339\u001b[0m     \"\"\"\n\u001b[1;32m   1340\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn.functional.tanh is deprecated. Use torch.tanh instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 3.95 GiB total capacity; 2.09 GiB already allocated; 89.06 MiB free; 2.14 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Load in the model from a file\n",
    "# load_models(\"./contentEncoder/saved_models/deepspeech5.pt\", \"./contentEncoder/saved_models/deepspeech6.pt\", 8)\n",
    "\n",
    "# Load a fresh model, untrained\n",
    "load_models()\n",
    "train()\n",
    "# output = encode(\"./data/contentAudio/40-222-0030.flac\")\n",
    "\n",
    "# Train the model\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "# Currently only one style encoding, generating all speech in same voice\n",
    "preprocessed_wav = styleEncoder.preprocess_wav(Path(\"data/styleAudio/rand1.flac\"))\n",
    "styleEmbed = torch.from_numpy(styleEncoder.embed_utterance(preprocessed_wav))\n",
    "styleEmbed = styleEmbed.repeat(_hparams['batch_size'],1).to(_device)\n",
    "print(styleEmbed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 600, 256])\n"
     ]
    }
   ],
   "source": [
    "tens = torch.empty((8, 600, 256))\n",
    "tens[1,:]=styleEmbed[1,:].repeat(1,600,1)\n",
    "print(tens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor((8,600,256)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "228",
   "language": "python",
   "name": "228"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
