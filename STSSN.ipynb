{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjorn/dev/ECE_228/228/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/bjorn/dev/ECE_228/228/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from STSSN.synthesizer import Synthesizer\n",
    "from STSSN.contentEncoder import ContentEncoder\n",
    "from STSSN.LibriStyleDataset import LibriStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = None # type: VoiceImpersonator\n",
    "_hparams = None\n",
    "_device = None\n",
    "_use_cuda = None\n",
    "_train_audio_transforms = None\n",
    "_valid_audio_transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_path=None, save_model_path=None, batch_size=8, epochs=1):\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "    _use_cuda = torch.cuda.is_available()\n",
    "    _device = torch.device(\"cuda\" if _use_cuda else \"cpu\")\n",
    "    warm_start=True\n",
    "\n",
    "    _hparams = {\n",
    "        \"n_cnn_layers\": 2,\n",
    "        \"n_rnn_layers\": 4,\n",
    "        \"rnn_dim\": 256,\n",
    "        \"n_feats\": 80,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"save_model_path\": save_model_path\n",
    "    }\n",
    "\n",
    "    _model = VoiceImpersonator(\n",
    "    _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "    _hparams['stride'], _hparams['dropout']\n",
    "    ).to(_device)\n",
    "\n",
    "    if model_path is not None:\n",
    "        _model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "    print(_model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in _model.parameters()]))\n",
    "    load_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge encoder and synthesizer into full network\n",
    "class VoiceImpersonator(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, stride=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=ContentEncoder(\n",
    "            _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "            _hparams['stride'], _hparams['n_feats'], _hparams['dropout']\n",
    "            )\n",
    "                \n",
    "        self.synthesizer=Synthesizer()\n",
    "        \n",
    "    def forward(self, spectrograms, styles):\n",
    "#         # Input is a 2 element list, containing a batch of spectrograms and a batch of style embeddings\n",
    "#         # Extract style embeddings from input tensor, remove from list\n",
    "#         embeds = x[1]\n",
    "#         x = x[0]\n",
    "        \n",
    "        # Compute content encoding\n",
    "        x = self.encoder(spectrograms)\n",
    "        \n",
    "        styleEmbeds = torch.empty((_hparams['batch_size'], x.shape[1], 256)).to(_device)\n",
    "        \n",
    "        for i in (0, styles.shape[0]-1):\n",
    "                styleEmbeds[i,:,:]=styles[i,:].repeat(1,x.shape[1],1)\n",
    "    \n",
    "        # Concatenate content with style embedding, now size (batch, timesteps, 768)\n",
    "        x = torch.cat((x, styleEmbeds), 2)\n",
    "        \n",
    "        print(\"Embedding Shape\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Synthesize a spectrogram from the combined embeddings\n",
    "        x = self.synthesizer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage data preprocessing (creating spectrograms, transforms, etc)\n",
    "def load_transforms():\n",
    "    global _train_audio_transforms, _valid_audio_transforms\n",
    "    _train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "    )\n",
    "\n",
    "    _valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80)\n",
    "    \n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    global _train_audio_transforms, _valid_audio_transforms\n",
    "    spectrograms = []\n",
    "    styles = []\n",
    "\n",
    "    # waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id\n",
    "    for (c, (wav, embed)) in enumerate(data, 1):\n",
    "        if data_type == 'train':\n",
    "            spec = _train_audio_transforms(wav).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = _valid_audio_transforms(wav).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "            \n",
    "        spectrograms.append(spec)\n",
    "        styles.append(np.expand_dims(embed, 1))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    styles = torch.from_numpy(np.concatenate(styles, 1)).transpose(0,1)\n",
    "    \n",
    "    return spectrograms, styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Behavior       \n",
    "def train():\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded, call load_model() before training\")\n",
    "\n",
    "    dataset_dir = os.path.expanduser(\"~/dev/datasets/Libri\")\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "\n",
    "    train_url=\"train-clean-100\"\n",
    "    test_url=\"test-clean\"\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if _use_cuda else {}\n",
    "    \n",
    "    train_dataset = LibriStyle(dataset_dir, url=train_url, download=True, preprocess=False)\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, batch_size=_hparams['batch_size'], shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    \n",
    "    test_dataset = LibriStyle(dataset_dir, url=test_url, download=True, preprocess=False)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset, batch_size=_hparams['batch_size'], shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "    optimizer = optim.AdamW(_model.parameters(), _hparams['learning_rate'])\n",
    "\n",
    "    # This is a pretty poor loss function for now\n",
    "    lossFunction = nn.MSELoss()      \n",
    "                             \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=_hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=_hparams['epochs'],\n",
    "                                            anneal_strategy='linear')       \n",
    "    \n",
    "    _model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for epoch in range(1, _hparams['epochs'] + 1):\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms, styles = _data\n",
    "\n",
    "            spectrograms = spectrograms.to(_device)\n",
    "            styles = styles.to(_device)\n",
    "            \n",
    "            print(\"Input Spectrograms Shape\")\n",
    "            print(spectrograms.shape)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = _model(spectrograms, styles)  # (batch, timesteps, frequencies)\n",
    "            \n",
    "            print(\"Output Spectrograms Shape\")\n",
    "            print(output.shape)\n",
    "#             print(spectrograms.shape)\n",
    "#             print(styles.shape)\n",
    "            \n",
    "#             loss = lossFunction(spectrograms, spectrograms)\n",
    "#             loss.backward()\n",
    "            \n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "                             \n",
    "#             # NEED TO IMPLEMENT NEW LOSS AND GRADIENT PROPOGATION\n",
    "                             \n",
    "# #             loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#             loss.backward()\n",
    "\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "#                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, batch_idx * len(spectrograms), data_len,\n",
    "#                     100. * batch_idx / len(train_loader), loss.item()))\n",
    "                             \n",
    "#         # Calculate validation statistics and save after each epoch                 \n",
    "#         test(_model, _device, test_loader, criterion, epoch)\n",
    "#         torch.save(_model.state_dict(), _hparams['save_model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Validation Metrics and Behavior (Currently Broken)\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms = _data \n",
    "            spectrograms = spectrograms.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "#             output = F.log_softmax(output, dim=2)\n",
    "#             output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            # Print loss information\n",
    "            \n",
    "#             loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "#             test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated, leaving in for possibility of code reuse in development             \n",
    "def encode(input_file_path):\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded, call load_model() and truncate_model() before encoding\")\n",
    "\n",
    "    # Currently only one style encoding, generating all speech in same voice\n",
    "    preprocessed_wav = styleEncoder.preprocess_wav(Path(\"data/styleAudio/rand1.flac\"))\n",
    "    styleEmbeds = torch.from_numpy(styleEncoder.embed_utterance(preprocessed_wav)).unsqueeze(0).to(_device)\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(input_file_path, normalization=True)\n",
    "    input_data = [[waveform, None, None, None, None, None]]\n",
    "    spectrograms = data_processing(input_data, 'valid').to(_device)\n",
    "    \n",
    "    _model.eval()\n",
    "    output=_model(spectrograms, styleEmbeds)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoiceImpersonator(\n",
      "  (encoder): ContentEncoder(\n",
      "    (Convolutional_Feature_Extraction): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Residual_CNN_Blocks): Sequential(\n",
      "      (0): ResidualCNN(\n",
      "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (layer_norm2): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualCNN(\n",
      "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (layer_norm2): CNNLayerNorm(\n",
      "          (layer_norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Feature_Downsampling): Linear(in_features=2560, out_features=256, bias=True)\n",
      "    (Recurrent_Block): Sequential(\n",
      "      (0): BidirectionalGRU(\n",
      "        (BiGRU): GRU(256, 256, batch_first=True, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): BidirectionalGRU(\n",
      "        (BiGRU): GRU(512, 256, bidirectional=True)\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (synthesizer): Synthesizer(\n",
      "    (prenet): PreNet(\n",
      "      (linear1): Linear(in_features=80, out_features=256, bias=True)\n",
      "      (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (dropout2): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (lstm): LSTM(1024, 1024, num_layers=2, batch_first=True)\n",
      "    (attention): LocationSensitiveAttention(\n",
      "      (W): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      (V): Linear(in_features=768, out_features=128, bias=False)\n",
      "      (U): Linear(in_features=32, out_features=128, bias=False)\n",
      "      (F): Conv1d(1, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
      "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
      "    )\n",
      "    (linear_proj): Linear(in_features=1024, out_features=80, bias=True)\n",
      "    (stop_token): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (postnet): PostNet(\n",
      "      (cnn1): Conv2d(1, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn2): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn3): Conv2d(256, 256, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (cnn4): Conv2d(256, 1, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 22891378\n",
      "Input Spectrograms Shape\n",
      "torch.Size([1, 1, 80, 1103])\n",
      "Embedding Shape\n",
      "torch.Size([1, 1103, 768])\n",
      "step input size\n",
      "torch.Size([1, 256])\n",
      "attention size\n",
      "torch.Size([1, 768])\n",
      "decoder_state torch.Size([1, 1024])\n",
      "step_attention torch.Size([1, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 1792], m2: [1024 x 80] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-934ce87b165f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load a fresh model, untrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# output = encode(\"./data/contentAudio/40-222-0030.flac\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f1edc3e49aee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, timesteps, frequencies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output Spectrograms Shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-99634fc203c4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, spectrograms, styles)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Synthesize a spectrogram from the combined embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/Real-Time-Voice-Cloning/STSSN/synthesizer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Concatenate lstm output with attention, project down to spectrogram output shape & scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mlinear_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mstop_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/ECE_228/228/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 1792], m2: [1024 x 80] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "# Load in the model from a file\n",
    "# load_models(\"./contentEncoder/saved_models/deepspeech5.pt\", \"./contentEncoder/saved_models/deepspeech6.pt\", 8)\n",
    "\n",
    "# Load a fresh model, untrained\n",
    "load_models(batch_size=1, epochs=1)\n",
    "train()\n",
    "# output = encode(\"./data/contentAudio/40-222-0030.flac\")\n",
    "\n",
    "# Train the model\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "228",
   "language": "python",
   "name": "228"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
