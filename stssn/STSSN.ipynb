{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = None # type: VoiceImpersonator\n",
    "_hparams = None\n",
    "_device = None\n",
    "_use_cuda = None\n",
    "_train_audio_transforms = None\n",
    "_valid_audio_transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path=None, save_model_path=None, batch_size=8):\n",
    "    global _model, _device, _hparams, _use_cuda, _train_audio_transforms, _valid_audio_transforms\n",
    "    _use_cuda = torch.cuda.is_available()\n",
    "    _device = torch.device(\"cuda\" if _use_cuda else \"cpu\")\n",
    "    warm_start=True\n",
    "\n",
    "    _hparams = {\n",
    "        \"n_cnn_layers\": 2,\n",
    "        \"n_rnn_layers\": 4,\n",
    "        \"rnn_dim\": 512,\n",
    "#         \"n_class\": 29,\n",
    "#         \"n_feats\": 64,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": 10,\n",
    "        \"save_model_path\": save_model_path\n",
    "    }\n",
    "\n",
    "    _model = VoiceImpersonator(\n",
    "    _hparams['n_cnn_layers'], _hparams['n_rnn_layers'], _hparams['rnn_dim'],\n",
    "    _hparams['stride'], _hparams['dropout']\n",
    "    ).to(_device)\n",
    "\n",
    "    if model_path is not None:\n",
    "        _model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "    print(_model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in _model.parameters()]))\n",
    "    \n",
    "    _train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "    )\n",
    "\n",
    "    _valid_audio_transforms = torchaudio.transforms.MelSpectrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blocks of layers and full model\n",
    "class VoiceImpersonator(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, stride=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Single convolutional layer for basic heirarchal feature extraction\n",
    "        # Conv2D(in_channels, out_channels, kernel_size, stride, padding, dilation....)\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=1)\n",
    "\n",
    "        # n_cnn Residual Convolutional Layers for deeper feature extraction\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=64) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "                             \n",
    "        # Single fully connected layer, 2048 inputs (64 features * 32 filters), rnn_dim outputs\n",
    "        self.fully_connected = nn.Linear(2048, rnn_dim)\n",
    "        \n",
    "        \n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage data preprocessing (creating spectrograms, transforms, etc)\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    global _train_audio_transforms, _valid_audio_transforms\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    \n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'infer':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "            spectrograms.append(spec)\n",
    "            continue\n",
    "        elif data_type == 'train':\n",
    "            spec = _train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = _valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    \n",
    "    if data_type != 'infer':\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Behavior       \n",
    "def train():\n",
    "    global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded, call load_model() before training\")\n",
    "\n",
    "    dataset_dir = os.path.expanduser(\"~/dev/datasets/Libri\")\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        print(\"Running\")\n",
    "        os.makedirs(dataset_dir)\n",
    "\n",
    "    train_url=\"train-clean-100\"\n",
    "    test_url=\"test-clean\"\n",
    "\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(dataset_dir, url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(dataset_dir, url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if _use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=_hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=_hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "    optimizer = optim.AdamW(_model.parameters(), _hparams['learning_rate'])\n",
    "#     criterion = nn.CTCLoss(blank=28).to(_device)\n",
    "            \n",
    "                             \n",
    "                             \n",
    "    # NEED A NEW LOSS\n",
    "                             \n",
    "                             \n",
    "                             \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=_hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=_hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "          \n",
    "                             \n",
    "    _model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for epoch in range(1, _hparams['epochs'] + 1):\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            torch.cuda.empty_cache()\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(_device), labels.to(_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = _model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                             \n",
    "            # NEED TO IMPLEMENT NEW LOSS AND GRADIENT PROPOGATION\n",
    "                             \n",
    "#             loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "                             \n",
    "        # Calculate validation statistics and save after each epoch                 \n",
    "        test(_model, _device, test_loader, criterion, epoch)\n",
    "        torch.save(_model.state_dict(), _hparams['save_model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Validation Metrics and Behavior\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "               \n",
    "                             \n",
    "# def encode(input_file_path):\n",
    "#     global _model, _device, _hparams, _use_cuda\n",
    "\n",
    "#     if _model is None:\n",
    "#         raise Exception(\"Model was not loaded, call load_model() and truncate_model() before encoding\")\n",
    "\n",
    "#     waveform, sample_rate = torchaudio.load(input_file_path, normalization=True)\n",
    "#     input_data = [[waveform, None, None, None, None, None]]\n",
    "#     input_layer = data_processing(input_data, 'infer')\n",
    "\n",
    "#     _model.eval()\n",
    "#     output=_model(input_layer[0].to(_device))\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model from a file\n",
    "load_model(\"./contentEncoder/saved_models/deepspeech5.pt\", \"./contentEncoder/saved_models/deepspeech6.pt\", 8)\n",
    "\n",
    "# Train the model\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "228",
   "language": "python",
   "name": "228"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
